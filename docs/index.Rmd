---
title: "TITLE"
subtitle: "Decoding Biases in AI: Group Project"
author: "Tom Akhurst, Marcelina Przespolewska, Aahil Sheikh, James Purnomo"
date: 'December 2, 2022'
output: html_document
---

## Introduction

The American polity is more polarized than ever, and more digitalized too. On one measurement of polarization, voters in the United States are now 56 points "colder" in sentiment towards supporters of opposing parties than supporters of their own, up from 27 points in 1978 (The Economist, 2021). Simultaneously, the number of American adults who "use the internet" has soared from 52% in 2000 to 93% in 2021 (Pew Research Center, 2021). This concurrence begs a classic question of "correlation or causation." More specifically, policy pundits are debating whether the increasing concentration of political and social life online has created polarization anew in the US, or whether it is simply playing host to an independent social trend. This question forms the broad focus of our investigation. 

Digitalization is a multifaceted phenomena with many elements, but one aspect of particular interest to the context of polarization is recommendation algorithms. These algorithms feature across the online world, on e-commerce websites, digital newspapers, social media, and more. While they diverge in their technical constitution, they each share a primary intention: to retain and shape user engagement by directing users to particular content (Covington et al., 2016). Given their key role in mediating the user experience online, recommendation algorithms are an appropriate object through which we can attempt to identify a potential causal relationship between digitalization and political polarization. 

For the sake of brevity, this paper narrows its focus to YouTube’s recommendation algorithm as a source of insight into the broader universe of online recommendation algorithms. This approach is informed by YouTube’s prominence as a source of political information online, and its significant user base (English, et al., 2011). 

Our research requires a point of orientation around which we can code recommended YouTube content as “polarizing” or “non-polarizing”. Polarization, as a concept, is inherently fluid and depends on the opinions or perspectives considered particularly divisive for a given society at a given time. Given that our research is focused on the American context, we have opted to code YouTube content as “polarizing” or “non-polarizing” based on its relationship to the ongoing debate over election integrity. Belief in the legitimacy and integrity of America’s election system is much higher among Democratic voters than Republican voters, and hence, disagreement in this space is currently a pertinent force behind polarization (Grant et al., 2021). Further, data analytics organization FiveThirtyEight identifies this debate as a key point of radicalization within the Republican Party, where Republicans are now judged as moderate or radical based on their support for Donald Trump’s accusations of voter fraud (2022). Our investigation is thereby concerned with whether YouTube's algorithm is directing users towards more radical right-wing content, as defined by support for political candidates, figures, organizations, and movements that openly favor conspriacy theories supporting election denial.

Our investigation, then, is directed by the following research question: to what extent does YouTube’s recommendation algorithm systematically lead users towards more polarizing right-wing content, when the algorithm is deprived of personalization data?

## Literature Review

Prior research on YouTube’s recommendation algorithm and its role in steering users towards more extreme content is relatively inconsistent. Yesilada and Lewandowsky (2022) conducted a review of existing research on YouTube’s recommendation system and found that around 60% of studies implicated YouTube’s algorithm in radicalization, while 30% produced mixed results, and less than 10% did not implicate the algorithm. Among those studies that identified a positive association, Haroon et al. (2022) attempted to audit YouTube’s recommendation system for both right-wing and left-wing radicalization and found that YouTube encourages far-right content to a greater extent than far-left content. Ribeiro et al. (2022) similarly argued that YouTube’s algorithm encourages radicalization among users, and that its recommendation system enables polarizing channels “to be discovered, even in a scenario without personalization.” Bisbee et al. (2022) found that YouTube’s algorithm promotes radicalization of political views, as users skeptical of the legitimacy of the 2020 presidential election were more likely to be provided with radicalized content. However, there also exist studies that dispute these claims of polarizing effects from YouTube’s recommendation algorithm-this includes research by Ledwich and Zaitsev (2020), which found that YouTube’s recommendation system disfavors radicalizing content.

Existing research on the polarizing effects of YouTube’s recommendation algorithm has been far from homogeneous in its methodology. Haroon et al. (2022), for example, trained artificial entities to watch a series of selected left-leaning and right-leaning videos, which were assigned to a score ranging from -1 (far-left) and to +1 (far-right) based on the ratio of accounts sharing the video. Ribeiro et al. (2022) manually assigned channels to chosen communities, such as “Alt-lite,” after watching each channel. Ledwich and Zaitsev (2020) similarly labeled content by watching channels “until the labelers found enough evidence for assigning specific labels.” Bisbee et al. (2022) “used unsupervised topic models of the video metadata (title, description, and video tags) to characterize the content of the recommended videos shown to [the] respondents”; they classified the content by using both Latent Dirichelet Allocation (LDA) and manual allocation, where they assigned labels based on whether each video was supportive of election fraud claims.


## Data Collection and Methodology

1) To evaluate YouTube's recommendation algorithm, we first selected two videos that would serve as "starting points" for our analysis of recommended content. Since this study is interested in polarization as it relates to ongoing political divisions in the US over election integrity, we selected videos commenting on a 2022 debate between two candidates for U.S. Senate in a key state of Pennsylvania: John Fetterman (D), who had criticized election conspiracy theories (Sharma, 2020), and Mehmet Oz (R), who had "raised questions" about the integrity of America's electoral system (FiveThirtyEight Staff, 2022). One video came from a left-leaning news source (MSNBC), and the other from a right-leaning news source (Fox News) (Mitchell et al., 2014).

2) To understand how the video recommendation algorithm works, we created a sample of 24 "cold starts," or recommendation chains that followed from each starting video described above; 12 recommendation chains followed from the MSNBC source, and 12 recommendation chains followed from the Fox News source. For each cold start, we utilized private browsing with deleted history and cookies in order to exclude any preexisting personalization data from inputting or otherwise affecting the behavior of the recommendation algorithm.

3) We tested the recommendation algorithm by watching six videos in succession following from the relevant "cold start" video. Each successive video was selected from the first clip featured in the recommendation panel. Each video was watched for 10 minutes, or otherwise until the clip expired. Our sample of 24 "cold starts," divided into 12 beginning with the MSNBC video and 12 beginning with the Fox News video, collectively amounted to 144 videos watched in total.

4) Similarly to Ribeiro et al. (2022) and Ledwich and Zaitsev (2022), we collected data by watching and manually labeling each video. For the purpose of this study, we created two measures: 1) support for candidates, public figures, organizations, movements, or other entities in favor of election fraud conspiracy theories and 2) support for claims in favor of election fraud conspiracy theories. Based on content, title, and author, we labeled each of the videos as follows (for the previously described two measures separately): 1) **1** for videos *supporting* candidates/entities or claims in favor of election fraud conspiracy theories, 2) **0** for videos *neutral* towards candidates/entities or claims in favor of election fraud conspiracy theories, and 3) **-1** for videos *criticizing* candidates/entities or claims in favor of election fraud conspiracy theories. Bisbee et al. (2022) similarly used election fraud claims to evaluate videos, yet, unlike our study, their research utilized survey data on participants' political leanings (who acted as users), and the users watched a "seed of videos" rather than one as a starting point.

5) We further utilized exploratory data visualizations and descriptive statistics in order to measure and compare video support for election-denying claims/candidates between different recommendation chains that followed from sources with differing political leanings.

## Results and Discussion

We began our initial investigation with an exploratory data analysis. We created the 72 videos watched in total from the MSNBC cold start, against the 72 videos watched in total from the Fox News cold start. This allowed us to observe whether the starting points had any predictive effect on the volume of radical right-wing content subsequently recommended by algorithm.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Loading libraries
library(tidyverse)
library(ggplot2)
library(readxl)
# Loading dataset
yt_data <- read_excel("~/Desktop/yt_data.xlsx")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Re-coding data
yt_data_candidate <- yt_data %>%
  rename(Index = candidate_support)
yt_data_candidate$Index <- as.character(yt_data_candidate$Index)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Visualizing data
video_plot_candidate <- ggplot(data = yt_data_candidate, aes(x = starting_video, fill = Index)) + xlab("Political leaning of starting video") + ylab("Video count") + labs(title = "Figure 1: Video support for election-denying candidates/entities, by starting point") + geom_bar(position = "stack") + scale_fill_manual(values = c("cadetblue2", "khaki2", "coral1")) + theme_classic()
video_plot_candidate
```

As Figure 1. above shows, the number of videos *criticizing* election deniers (coded as -1) produced by the left-leaning starting point is greater than the number of videos *supporting* election deniers (coded as 1) that followed from the right-leaning starting point. Further, the proportion of neutral videos (coded as 0) appears to be greater for the right-leaning starting video than for the left-leaning starting video, suggesting that more conservative content is *less* likely to steer the users towards videos supporting election deniers than the more liberal content is towards videos supporting election *non*-deniers; this pattern is also similar for videos that directly support or criticize election fraud conspiracy theories, as presented in Figure 2. below.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Re-coding data
yt_data_claim <- yt_data %>%
  rename(Index = claim_support)
yt_data_claim$Index <- as.character(yt_data_claim$Index)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Visualizing data
video_plot_claim <- ggplot(data = yt_data_claim, aes(x = starting_video, fill = Index)) + xlab("Political leaning of starting video") + ylab("Video count") + labs(title = "Figure 2: Video support for election-denying claims, by starting point") + geom_bar(position = "stack") + scale_fill_manual(values = c("cadetblue2", "khaki2", "coral1")) + theme_classic()
video_plot_claim
```

As shown above, the number of videos supporting election-denying claims that followed from the right-leaning starting point is substantially smaller than the proportion of videos criticizing election-denying claims that followed from the left-leaning starting point, further indicating that left-leaning content, rather than right-leaning content, might be more likely to produce biased videos.

This initial analysis, however, does not take into account differences between recommendation chains. For this reason, it might be helpful to evaluate how the results compare between different "cold starts."

The table below presents the mean measure of support for election-denying candidates/entities (which we will call the "candidate index") among different recommendation chains (cold starts) that followed from the left-leaning starting video.


```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Creating new datasets
yt_data_L_candidate <- filter(yt_data, starting_video == "L")
yt_data_R_candidate <- filter(yt_data, starting_video == "R")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Wrangling left/candidate dataset
yt_data_L_candidate <- yt_data_L_candidate %>%
  group_by(cold_start) %>%
  summarize(
    candidate_index = mean(candidate_support)
  )
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Displaying left/candidate dataset
yt_data_L_candidate
```

As the table above shows, most recommendation chains generated by the left-leaning starting point and composed of 6 videos, not surprisingly, have the index below 0, indicating that the recommendation algorithm overwhelmingly steered the user clicking through a recommendation chain towards content criticizing election-denying candidates or entities. However, there are some anomalies, such as recommendation chain #1, #4, and #8. The pattern is similar for the mean measure of support for election-denying claims, called "claim index" for the purpose of this study, as presented in the table below.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Creating new datasets
yt_data_L_claim <- filter(yt_data, starting_video == "L")
yt_data_R_claim <- filter(yt_data, starting_video == "R")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Wrangling left/claim dataset
yt_data_L_claim <- yt_data_L_claim %>%
  group_by(cold_start) %>%
  summarize(
    claim_index = mean(claim_support)
  )
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Displaying left/claim dataset
yt_data_L_claim
```

The same indexes for recommendation chains that started from the right-leaning starting point, however, show a slightly different pattern, as presented in the two tables below.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Wrangling right/candidate dataset
yt_data_R_candidate <- yt_data_R_candidate %>%
  group_by(cold_start) %>%
  summarize(
    candidate_index = mean(candidate_support)
  )
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Displaying right/candidate dataset
yt_data_R_candidate
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Wrangling right/claim dataset
yt_data_R_claim <- yt_data_R_claim %>%
  group_by(cold_start) %>%
  summarize(
    claim_index = mean(claim_support)
  )
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Displaying right/claim dataset
yt_data_R_claim
```

Here, all recommendation chains composed of 6 videos that followed from the right-leaning starting point have the mean support for election-denying candidates/entities above 0, indicating that, overall, these chains, unlike those for left-leaning starting point, might have produced more biased content. For election-denying claims, however, the right-leaning video-produced chains appear to be more neutral, with many chains having the claim index of 0. 

However, when we average all recommendation chain indexes for left-leaning and right-leaning starting points, separately, the results show that chains originating from the left-leaning starting video have roughly the same index as chains originating from the right-leaning starting video in absolute terms. The claim index average remains slightly higher for the left-leaning starting point, again, in absolute terms.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Producing total means
yt_data_candidate_index <- yt_data %>%
  group_by(cold_start) %>%
  group_by(starting_video) %>%
  summarize(
    candidate_index = mean(candidate_support)
  )
yt_data_claim_index <- yt_data %>%
  group_by(cold_start) %>%
  group_by(starting_video) %>%
  summarize(
    claim_index = mean(claim_support)
  )
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
yt_data_candidate_index
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
yt_data_claim_index
```

Finally, **more than half** of the recommendation chains that followed from the right-leaning starting video began showing more neutral content *after* showing one, two, or three videos supporting candidates/entities denying the legitimacy of election results. An example of one such recommendation chain can be seen below.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Loading dataset
extracted_yt_data_sample1 <- read_excel("~/Desktop/extracted_yt_data_sample1.xlsx")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Displaying dataset
extracted_yt_data_sample1
```

As such, the overall results appear to be mixed and thus did not produce enough evidence to support the claim that the YouTube recommendation algorithm steers its users towards far-right content, defined as content supporting candidates or claims in favor of election fraud conspiracy theories,  more aggressively than it does towards content criticizing said theories. In fact, the results appear to be more suggestive of the opposite.
